{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import threading\n",
    "import cv2\n",
    "import queue\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "def mediapipe_detection(image, holistic_model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = holistic_model.process(image)        # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def extract_hand_keypoints(results):\n",
    "    if results.left_hand_landmarks:\n",
    "        lh = np.array([[res.x, res.y] for res in results.left_hand_landmarks.landmark])\n",
    "    else:\n",
    "        lh = np.zeros((21, 2))\n",
    "    \n",
    "    if results.right_hand_landmarks:\n",
    "        rh = np.array([[res.x, res.y] for res in results.right_hand_landmarks.landmark])\n",
    "    else:\n",
    "        rh = np.zeros((21, 2))  \n",
    "    return lh, rh\n",
    "\n",
    "def extract_index_keypoints(results):\n",
    "    if results.left_hand_landmarks:\n",
    "        index_finger = results.left_hand_landmarks.landmark[8]\n",
    "        return np.array([index_finger.x, index_finger.y])  # Tọa độ x, y của ngón trỏ\n",
    "    else:\n",
    "        return np.zeros(2)\n",
    "\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idle': 0, 'up': 1, 'down': 2, 'left': 3, 'right': 4}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_direction = ['idle', 'up', 'down', 'left', 'right']\n",
    "\n",
    "label_map_dir = {label:num for num, label in enumerate(classes_direction)}\n",
    "label_map_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movement': 0, 'other': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_movement = ['movement', 'other']\n",
    "\n",
    "label_map_check = {label:num for num, label in enumerate(classes_movement)}\n",
    "label_map_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idle': 0, 'chop': 1, 'pinch': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_interaction = ['idle', 'chop', 'pinch']\n",
    "\n",
    "label_map_interact = {label:num for num, label in enumerate(classes_interaction)}\n",
    "label_map_interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_points(points):\n",
    "    origin = points[0]\n",
    "    \n",
    "    # Chuẩn hóa tọa độ bằng cách trừ tọa độ frame đầu tiên\n",
    "    normalized_points = points - origin\n",
    "    \n",
    "    return normalized_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_keypoints(keypoints):\n",
    "    # Kiểm tra nếu có đủ số điểm (21 điểm cho mỗi bàn tay)\n",
    "    if keypoints.shape[0] != 21:\n",
    "        raise ValueError(f\"Số lượng điểm keypoints không hợp lệ: {keypoints.shape[0]}\")\n",
    "\n",
    "    # Cổ tay là điểm đầu tiên trong keypoints (index 0)\n",
    "    wrist = keypoints[0]\n",
    "    \n",
    "    # Dịch các điểm sao cho cổ tay trở thành gốc tọa độ (0, 0)\n",
    "    normalized_keypoints = []\n",
    "    for point in keypoints:\n",
    "        normalized_point = (point[0] - wrist[0], point[1] - wrist[1])  # Chỉ cần dịch x, y\n",
    "        normalized_keypoints.append(normalized_point)\n",
    "    \n",
    "    # Chuyển sang numpy array để dễ dàng tính toán min và max\n",
    "    normalized_keypoints = np.array(normalized_keypoints)\n",
    "    \n",
    "    # Tính toán min và max cho x và y\n",
    "    x_min, y_min = np.min(normalized_keypoints, axis=0)\n",
    "    x_max, y_max = np.max(normalized_keypoints, axis=0)\n",
    "    \n",
    "    # Tránh chia cho 0 nếu max - min = 0\n",
    "    if (x_max - x_min) == 0:\n",
    "        x_min, x_max = 0, 1  \n",
    "    if (y_max - y_min) == 0:\n",
    "        y_min, y_max = 0, 1\n",
    "    \n",
    "    # Chuyển min và max về dạng numpy array để có thể tính toán đúng\n",
    "    min_vals = np.array([x_min, y_min])\n",
    "    max_vals = np.array([x_max, y_max])\n",
    "    \n",
    "    normalized_keypoints = (normalized_keypoints - min_vals) / (max_vals - min_vals)\n",
    "    \n",
    "    return normalized_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1344</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1344</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">43,040</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m192\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1344\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1344\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m43,040\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">129,797</span> (507.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m129,797\u001b[0m (507.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,265</span> (169.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m43,265\u001b[0m (169.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">86,532</span> (338.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m86,532\u001b[0m (338.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_check = tf.keras.models.load_model('movement_check.keras')\n",
    "model_check.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m4,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m24,832\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m165\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">94,673</span> (369.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m94,673\u001b[0m (369.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,557</span> (123.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,557\u001b[0m (123.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">63,116</span> (246.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m63,116\u001b[0m (246.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_direction = tf.keras.models.load_model('movement_direction.keras')\n",
    "model_direction.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m9,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m24,832\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m99\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,835</span> (429.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,835\u001b[0m (429.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">36,611</span> (143.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m36,611\u001b[0m (143.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">73,224</span> (286.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m73,224\u001b[0m (286.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_interact = tf.keras.models.load_model('interaction.keras')\n",
    "model_interact.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_check.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_direction.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_interact.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP = '127.0.0.1'\n",
    "PORT = 25001\n",
    "\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "\n",
    "def SendData(message):\n",
    "    s.sendto(message.encode(), (IP, PORT))\n",
    "    print(f\"{message} sent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. New detection variables\n",
    "# movement_seq = []\n",
    "# interaction_seq = []\n",
    "\n",
    "# threshold = 0.7\n",
    "\n",
    "# frame_counter = 0  # Khởi tạo biến đếm\n",
    "\n",
    "# isMovement = 0\n",
    "# predictions_direction = 0\n",
    "# predictions_interaction = 0\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# # Set mediapipe model \n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#     while cap.isOpened():\n",
    "        \n",
    "#         # Read feed\n",
    "#         ret, frame = cap.read()\n",
    "        \n",
    "#         frame_counter += 1 \n",
    "\n",
    "#         # Make detections\n",
    "#         image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "#         # Draw landmarks\n",
    "#         draw_styled_landmarks(image, results)\n",
    "        \n",
    "#         lh_keypoints, rh_keypoints = extract_hand_keypoints(results)\n",
    "#         index_keypoints = extract_index_keypoints(results)\n",
    "\n",
    "#         movement_seq.append(index_keypoints)\n",
    "        \n",
    "#         # Interaction Predict\n",
    "#         rh_keypoints = normalize_keypoints(rh_keypoints)\n",
    "#         rh_keypoints = rh_keypoints.flatten()\n",
    "#         interaction_seq.append(rh_keypoints)\n",
    "#         interaction_seq = interaction_seq[-10:]\n",
    "#         if len(interaction_seq) == 10:\n",
    "#             res_interaction = model_interact.predict(np.expand_dims(interaction_seq, axis=0), verbose=0)[0]\n",
    "#             predictions_interaction = np.argmax(res_interaction)\n",
    "        \n",
    "#         # Movement check\n",
    "#         if frame_counter % 30 == 0:\n",
    "#             lh_keypoints = normalize_keypoints(lh_keypoints)\n",
    "#             lh_keypoints = np.expand_dims(lh_keypoints, axis=0)\n",
    "#             res_check = model_check.predict(lh_keypoints, verbose=0)\n",
    "#             isMovement = 0 if res_check[0] > 0.5 else 1\n",
    "        \n",
    "#         # if isMovement == 1:\n",
    "#         seq = np.array(movement_seq[-10:])\n",
    "#         seq = normalize_points(seq)\n",
    "        \n",
    "#         if len(seq) == 10:\n",
    "#             res_direction = model_direction.predict(np.expand_dims(seq, axis=0), verbose=0)[0]\n",
    "#             predictions_direction = np.argmax(res_direction)\n",
    "        \n",
    "        \n",
    "#         # Send data through UDP\n",
    "#         send_result = f\"{isMovement}, {predictions_direction}, {predictions_interaction}\"\n",
    "#         SendData(send_result)\n",
    "        \n",
    "#         # Show to screen\n",
    "#         cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "#         if frame_counter % 100 == 0:\n",
    "#             frame_counter = 0\n",
    "\n",
    "#         # Break gracefully\n",
    "#         if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#             break\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0, 0 sent\n",
      "0, 0, 2 sent\n",
      "0, 0, 1 sent\n",
      "0, 0, 2 sent\n",
      "0, 0, 0 sent\n",
      "0, 3, 0 sent\n",
      "0, 2, 0 sent\n",
      "0, 4, 0 sent\n",
      "0, 0, 0 sent\n",
      "1, 0, 0 sent\n",
      "1, 4, 0 sent\n",
      "1, 0, 0 sent\n",
      "1, 3, 0 sent\n",
      "1, 0, 0 sent\n",
      "1, 1, 0 sent\n",
      "1, 0, 0 sent\n",
      "1, 2, 0 sent\n",
      "1, 0, 0 sent\n",
      "1, 2, 0 sent\n"
     ]
    }
   ],
   "source": [
    "# Biến lưu trữ trạng thái trước đó\n",
    "prev_isMovement = None\n",
    "prev_predictions_direction = None\n",
    "prev_predictions_interaction = None\n",
    "\n",
    "# Các biến mới\n",
    "movement_seq = []\n",
    "interaction_seq = []\n",
    "threshold = 0.7\n",
    "frame_counter = 0  # Khởi tạo biến đếm\n",
    "isMovement = 0\n",
    "predictions_direction = 0\n",
    "predictions_interaction = 0\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        \n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        frame_counter += 1 \n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        lh_keypoints, rh_keypoints = extract_hand_keypoints(results)\n",
    "        index_keypoints = extract_index_keypoints(results)\n",
    "\n",
    "        movement_seq.append(index_keypoints)\n",
    "        \n",
    "        # Interaction Predict\n",
    "        rh_keypoints = normalize_keypoints(rh_keypoints)\n",
    "        rh_keypoints = rh_keypoints.flatten()\n",
    "        interaction_seq.append(rh_keypoints)\n",
    "        interaction_seq = interaction_seq[-10:]\n",
    "        if len(interaction_seq) == 10:\n",
    "            res_interaction = model_interact.predict(np.expand_dims(interaction_seq, axis=0), verbose=0)[0]\n",
    "            predictions_interaction = np.argmax(res_interaction)\n",
    "        \n",
    "        # Movement check\n",
    "        if frame_counter % 30 == 0:\n",
    "            lh_keypoints = normalize_keypoints(lh_keypoints)\n",
    "            lh_keypoints = np.expand_dims(lh_keypoints, axis=0)\n",
    "            res_check = model_check.predict(lh_keypoints, verbose=0)\n",
    "            isMovement = 0 if res_check[0] > 0.7 else 1\n",
    "        \n",
    "        # if isMovement == 1:\n",
    "        seq = np.array(movement_seq[-10:])\n",
    "        seq = normalize_points(seq)\n",
    "        \n",
    "        if len(seq) == 10:\n",
    "            res_direction = model_direction.predict(np.expand_dims(seq, axis=0), verbose=0)[0]\n",
    "            predictions_direction = np.argmax(res_direction)\n",
    "\n",
    "        # Kiểm tra sự thay đổi trước khi gửi dữ liệu\n",
    "        if (isMovement != prev_isMovement or\n",
    "            predictions_direction != prev_predictions_direction or\n",
    "            predictions_interaction != prev_predictions_interaction):\n",
    "            \n",
    "            # Gửi dữ liệu qua UDP nếu có sự thay đổi\n",
    "            send_result = f\"{isMovement}, {predictions_direction}, {predictions_interaction}\"\n",
    "            SendData(send_result)\n",
    "            \n",
    "            # Cập nhật giá trị trạng thái trước đó\n",
    "            prev_isMovement = isMovement\n",
    "            prev_predictions_direction = predictions_direction\n",
    "            prev_predictions_interaction = predictions_interaction\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if frame_counter % 100 == 0:\n",
    "            frame_counter = 0\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Thiết lập mô hình cho từng tác vụ\n",
    "# def predict_interaction(interaction_seq):\n",
    "#     # Interaction Predict\n",
    "#     rh_keypoints = normalize_keypoints(interaction_seq)\n",
    "#     rh_keypoints = rh_keypoints.flatten()\n",
    "#     res_interaction = model_interact.predict(np.expand_dims(interaction_seq, axis=0), verbose=0)[0]\n",
    "#     predictions_interaction = np.argmax(res_interaction)\n",
    "#     return predictions_interaction\n",
    "\n",
    "# def predict_movement_check(lh_keypoints):\n",
    "#     # Movement check\n",
    "#     lh_keypoints = normalize_keypoints(lh_keypoints)\n",
    "#     lh_keypoints = np.expand_dims(lh_keypoints, axis=0)\n",
    "#     res_check = model_check.predict(lh_keypoints, verbose=0)\n",
    "#     isMovement = 0 if res_check[0] > 0.5 else 1\n",
    "#     return isMovement\n",
    "\n",
    "# def predict_direction(movement_seq):\n",
    "#     # Direction Predict\n",
    "#     seq = np.array(movement_seq[-10:])\n",
    "#     seq = normalize_points(seq)\n",
    "#     res_direction = model_direction.predict(np.expand_dims(seq, axis=0), verbose=0)[0]\n",
    "#     predictions_direction = np.argmax(res_direction)\n",
    "#     return predictions_direction\n",
    "\n",
    "# # Thiết lập một hàm chung để chạy ba mô hình trên các luồng\n",
    "# def run_model_in_threads():\n",
    "#     interaction_seq = []\n",
    "#     movement_seq = []\n",
    "#     threshold = 0.7\n",
    "#     frame_counter = 0  # Khởi tạo biến đếm\n",
    "\n",
    "#     isMovement = 0\n",
    "#     predictions_direction = 0\n",
    "#     predictions_interaction = 0\n",
    "\n",
    "#     cap = cv2.VideoCapture(0)\n",
    "#     with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#         while cap.isOpened():\n",
    "#             ret, frame = cap.read()\n",
    "#             frame_counter += 1 \n",
    "\n",
    "#             # Make detections\n",
    "#             image, results = mediapipe_detection(frame, holistic)\n",
    "#             draw_styled_landmarks(image, results)\n",
    "\n",
    "#             lh_keypoints, rh_keypoints = extract_hand_keypoints(results)\n",
    "#             index_keypoints = extract_index_keypoints(results)\n",
    "\n",
    "#             movement_seq.append(index_keypoints)\n",
    "#             interaction_seq.append(rh_keypoints)\n",
    "\n",
    "#             # Cắt ngắn dữ liệu nếu quá dài\n",
    "#             interaction_seq = interaction_seq[-10:]\n",
    "#             if len(interaction_seq) == 10:\n",
    "#                 # Khởi tạo 3 luồng tương ứng với 3 mô hình\n",
    "#                 thread_interaction = threading.Thread(target=predict_interaction, args=(interaction_seq,))\n",
    "#                 thread_movement = threading.Thread(target=predict_movement_check, args=(lh_keypoints,))\n",
    "#                 thread_direction = threading.Thread(target=predict_direction, args=(movement_seq,))\n",
    "\n",
    "#                 # Bắt đầu các luồng\n",
    "#                 thread_interaction.start()\n",
    "#                 thread_movement.start()\n",
    "#                 thread_direction.start()\n",
    "\n",
    "#                 # Đợi tất cả các luồng hoàn thành\n",
    "#                 thread_interaction.join()\n",
    "#                 thread_movement.join()\n",
    "#                 thread_direction.join()\n",
    "\n",
    "#                 # Sau khi các luồng hoàn thành, nhận kết quả từ các mô hình\n",
    "#                 predictions_interaction = thread_interaction.result\n",
    "#                 isMovement = thread_movement.result\n",
    "#                 predictions_direction = thread_direction.result\n",
    "\n",
    "#                 # Gửi dữ liệu qua UDP nếu có sự thay đổi\n",
    "#                 send_result = f\"{isMovement}, {predictions_direction}, {predictions_interaction}\"\n",
    "#                 SendData(send_result)\n",
    "\n",
    "#             # Hiển thị kết quả\n",
    "#             cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "#             if frame_counter % 100 == 0:\n",
    "#                 frame_counter = 0\n",
    "\n",
    "#             if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#                 break\n",
    "#         cap.release()\n",
    "#         cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# run_model_in_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Thiết lập mô hình cho từng tác vụ\n",
    "# # def predict_interaction(interaction_seq, result_queue):\n",
    "# #     rh_keypoints = normalize_keypoints(interaction_seq)\n",
    "# #     rh_keypoints = rh_keypoints.flatten()\n",
    "# #     res_interaction = model_interact.predict(np.expand_dims(interaction_seq, axis=0), verbose=0)[0]\n",
    "# #     predictions_interaction = np.argmax(res_interaction)\n",
    "# #     result_queue.put(('interaction', predictions_interaction))\n",
    "    \n",
    "# def predict_interaction(interaction_seq, result_queue):\n",
    "#     # Chuyển đổi interaction_seq thành NumPy array trước khi sử dụng\n",
    "#     interaction_seq = np.array(interaction_seq)\n",
    "#     rh_keypoints = normalize_keypoints(interaction_seq)\n",
    "#     rh_keypoints = rh_keypoints.flatten()  # Làm phẳng để đưa vào mô hình\n",
    "#     res_interaction = model_interact.predict(np.expand_dims(rh_keypoints, axis=0), verbose=0)[0]\n",
    "#     predictions_interaction = np.argmax(res_interaction)\n",
    "#     result_queue.put(('interaction', predictions_interaction))\n",
    "\n",
    "# def predict_movement_check(lh_keypoints, result_queue):\n",
    "#     lh_keypoints = normalize_keypoints(lh_keypoints)\n",
    "#     lh_keypoints = np.expand_dims(lh_keypoints, axis=0)\n",
    "#     res_check = model_check.predict(lh_keypoints, verbose=0)\n",
    "#     isMovement = 0 if res_check[0] > 0.5 else 1\n",
    "#     result_queue.put(('movement', isMovement))\n",
    "\n",
    "# def predict_direction(movement_seq, result_queue):\n",
    "#     seq = np.array(movement_seq[-10:])\n",
    "#     seq = normalize_points(seq)\n",
    "#     res_direction = model_direction.predict(np.expand_dims(seq, axis=0), verbose=0)[0]\n",
    "#     predictions_direction = np.argmax(res_direction)\n",
    "#     result_queue.put(('direction', predictions_direction))\n",
    "\n",
    "# # Thiết lập một hàm chung để chạy ba mô hình trên các luồng\n",
    "# def run_model_in_threads():\n",
    "#     interaction_seq = []\n",
    "#     movement_seq = []\n",
    "#     threshold = 0.7\n",
    "#     frame_counter = 0  # Khởi tạo biến đếm\n",
    "\n",
    "#     isMovement = 0\n",
    "#     predictions_direction = 0\n",
    "#     predictions_interaction = 0\n",
    "\n",
    "#     cap = cv2.VideoCapture(0)\n",
    "#     with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#         while cap.isOpened():\n",
    "#             ret, frame = cap.read()\n",
    "#             frame_counter += 1 \n",
    "\n",
    "#             # Make detections\n",
    "#             image, results = mediapipe_detection(frame, holistic)\n",
    "#             draw_styled_landmarks(image, results)\n",
    "\n",
    "#             lh_keypoints, rh_keypoints = extract_hand_keypoints(results)\n",
    "#             index_keypoints = extract_index_keypoints(results)\n",
    "\n",
    "#             movement_seq.append(index_keypoints)\n",
    "#             interaction_seq.append(rh_keypoints)\n",
    "\n",
    "#             # Cắt ngắn dữ liệu nếu quá dài\n",
    "#             interaction_seq = interaction_seq[-10:]\n",
    "#             if len(interaction_seq) == 10:\n",
    "#                 # Khởi tạo queue để nhận kết quả từ các luồng\n",
    "#                 result_queue = queue.Queue()\n",
    "\n",
    "#                 # Khởi tạo 3 luồng tương ứng với 3 mô hình\n",
    "#                 thread_interaction = threading.Thread(target=predict_interaction, args=(interaction_seq, result_queue))\n",
    "#                 thread_movement = threading.Thread(target=predict_movement_check, args=(lh_keypoints, result_queue))\n",
    "#                 thread_direction = threading.Thread(target=predict_direction, args=(movement_seq, result_queue))\n",
    "\n",
    "#                 # Bắt đầu các luồng\n",
    "#                 thread_interaction.start()\n",
    "#                 thread_movement.start()\n",
    "#                 thread_direction.start()\n",
    "\n",
    "#                 # Đợi tất cả các luồng hoàn thành\n",
    "#                 thread_interaction.join()\n",
    "#                 thread_movement.join()\n",
    "#                 thread_direction.join()\n",
    "\n",
    "#                 # Nhận kết quả từ các luồng\n",
    "#                 while not result_queue.empty():\n",
    "#                     task, result = result_queue.get()\n",
    "#                     if task == 'interaction':\n",
    "#                         predictions_interaction = result\n",
    "#                     elif task == 'movement':\n",
    "#                         isMovement = result\n",
    "#                     elif task == 'direction':\n",
    "#                         predictions_direction = result\n",
    "\n",
    "#                 # Gửi dữ liệu qua UDP nếu có sự thay đổi\n",
    "#                 send_result = f\"{isMovement}, {predictions_direction}, {predictions_interaction}\"\n",
    "#                 SendData(send_result)\n",
    "\n",
    "#             # Hiển thị kết quả\n",
    "#             cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "#             if frame_counter % 100 == 0:\n",
    "#                 frame_counter = 0\n",
    "\n",
    "#             if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#                 break\n",
    "#         cap.release()\n",
    "#         cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# run_model_in_threads()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
